{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SpaCy\n",
    "The spaCy library is one of the most popular NLP libraries along with NLTK. The basic difference between the two libraries is the fact that NLTK contains a wide variety of algorithms to solve one problem whereas spaCy contains only one, but the best algorithm to solve a problem.\n",
    "\n",
    "NLTK was released back in 2001 while spaCy is relatively new and was developed in 2015. In this series of articles on NLP, we will mostly be dealing with spaCy, owing to its state of the art nature. However, we will also touch NLTK when it is easier to perform a task using NLTK rather than spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing spaCy\n",
    "If you use the pip installer to install your Python libraries, go to the command line and execute the following statement:\n",
    "\n",
    "    - pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/f2/052bfe5861761599b5421916aba3eb0064d83145ff3072390ecdc5a836de/spacy-2.2.3.tar.gz (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 215kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/db/bfae863870f79260e57e293dd835e848e8450d2a2c9e273795b13060ff86/blis-0.4.1-cp27-cp27mu-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.7MB 260kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/4c/0e0fa8b1e193c1e09a6b72807ff4ca17c78f68f0c0f4459bc8043c66d649/catalogue-0.2.0-py2.py3-none-any.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/8d/d095bbb109a004351c85c83bc853782fc27692693b305dd7b170c36a1262/cymem-2.0.3.tar.gz (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/31/247b34db5ab06afaf5512481e77860fb4cd7a0c0ddff9d2566651c8c2f07/murmurhash-1.0.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting plac<1.2.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/27/32d860083e0708e36a2266bed865dba4b55c991a84688932122e48ef65b4/preshed-3.0.2-cp27-cp27mu-manylinux1_x86_64.whl (113kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 5.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/54/28/c45d8b54c1339f9644b87663945e54a8503cfef59cf0f65b3ff5dd17cf64/setuptools-42.0.2-py2.py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=0.1.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/a4/f5fcb610d135b604937c4f2c8923ab524970b3ee88189e12ab9cd9696c64/srsly-0.2.0-cp27-cp27mu-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/24/ddef0f9d0c526d1eec76340a7c1299d6356b609b3cc00af602a335682905/thinc-7.3.1-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 578kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/c5/b999502e0b925d1f37730abd412ec2e626348040722e2c86d0f3f9dc898c/wasabi-0.4.2.tar.gz\n",
      "Collecting pathlib==1.0.1 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/e9/71/1a1e0ed0981bb6a67bce55a210f168126b7ebd2065958673797ea66489ca/importlib_metadata-1.3.0-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 6.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.10.0 (from thinc<7.4.0,>=7.3.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/c3/d049cf3fb31094ee045ec1ee29fffac218c91e82c8838c49ab4c3e52627b/tqdm-4.41.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 5.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\n",
      "Collecting configparser>=3.5; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Collecting contextlib2; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Collecting pathlib2; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
      "Collecting more-itertools (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/9d/dcfe59e213093695f108508af1214cf9cd95cc5489e46877ec5cb56369e5/more_itertools-5.0.0-py2-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six (from pathlib2; python_version < \"3\"->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting scandir; python_version < \"3.5\" (from pathlib2; python_version < \"3\"->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/f5/9c052db7bd54d0cbf1bc0bb6554362bba1012d03e5888950a4f5c5dadc4e/scandir-1.10.0.tar.gz\n",
      "Building wheels for collected packages: spacy, cymem, wasabi, pathlib, scandir\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/synerzip/.cache/pip/wheels/95/c8/b3/a1fa71c0ebfe632fa975b6d1c494924d8d180637754c3402de\n",
      "  Running setup.py bdist_wheel for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/synerzip/.cache/pip/wheels/ec/63/9a/dd916ca28285b94bd1f1afcbcd2bb65c2cae7ed2b84f565d43\n",
      "  Running setup.py bdist_wheel for wasabi ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/synerzip/.cache/pip/wheels/79/5e/3d/6f73130bb2ec8b1085fcd444442e5aa77888332820a9e072b8\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/synerzip/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for scandir ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/synerzip/.cache/pip/wheels/91/95/75/19c98a91239878abbc7c59970abd3b4e0438a7dd5b61778335\n",
      "Successfully built spacy cymem wasabi pathlib scandir\n",
      "Installing collected packages: numpy, blis, six, more-itertools, zipp, configparser, contextlib2, scandir, pathlib2, importlib-metadata, catalogue, cymem, murmurhash, plac, preshed, urllib3, certifi, chardet, idna, requests, setuptools, pathlib, srsly, tqdm, wasabi, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-0.2.0 certifi-2019.11.28 chardet-3.0.4 configparser-4.0.2 contextlib2-0.6.0.post1 cymem-2.0.3 idna-2.8 importlib-metadata-1.3.0 more-itertools-5.0.0 murmurhash-1.0.2 numpy-1.16.5 pathlib-1.0.1 pathlib2-2.3.5 plac-1.1.3 preshed-3.0.2 requests-2.22.0 scandir-1.10.0 setuptools-42.0.2 six-1.13.0 spacy-2.2.3 srsly-0.2.0 thinc-7.3.1 tqdm-4.41.0 urllib3-1.25.7 wasabi-0.4.2 zipp-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download spacy model\n",
    "Once you download and install spaCy, the next step is to download the language model. We will be using the English language model. The language model is used to perform a variety of NLP tasks, which we will see in a later section.\n",
    "\n",
    "The following command downloads the language model:\n",
    "\n",
    "    -python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, you need to import the spacy library as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the spaCy language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the load function from the spacy library to load the core English language model. The model is stored in the sp variable.\n",
    "\n",
    "Let's now create a small document using this model. A document can be a sentence or a group of sentences and can have unlimited length. The following script creates a simple spaCy document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Stackabuse.\n",
      "The site with the best Python Tutorials.\n",
      "What are you looking for?\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentence = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')\n",
    "for sentence in sentence.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "from\n",
      "Stackabuse\n",
      ".\n",
      "The\n",
      "site\n",
      "with\n",
      "the\n",
      "best\n",
      "Python\n",
      "Tutorials\n",
      ".\n",
      "What\n",
      "are\n",
      "you\n",
      "looking\n",
      "for\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "sentence = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "S\n",
      "t\n",
      "a\n",
      "c\n",
      "k\n",
      "a\n",
      "b\n",
      "u\n",
      "s\n",
      "e\n",
      ".\n",
      " \n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "t\n",
      "e\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "P\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      " \n",
      "T\n",
      "u\n",
      "t\n",
      "o\n",
      "r\n",
      "i\n",
      "a\n",
      "l\n",
      "s\n",
      ".\n",
      " \n",
      "W\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "sentence = 'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?'\n",
    "for word in sentence:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "from\n",
      "Stackabuse\n",
      ".\n",
      "The\n",
      "site\n",
      "with\n",
      "the\n",
      "best\n",
      "Python\n",
      "Tutorials\n",
      ".\n",
      "What\n",
      "are\n",
      "you\n",
      "looking\n",
      "for\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "sentence = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sentence[4].is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech (POS) Tagging\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = sp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print(doc.text)\n",
    "# doc = (\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How ---> ADV\n",
      "wh-adverb\n",
      "are ---> AUX\n",
      "verb, non-3rd person singular present\n",
      "you ---> PRON\n",
      "pronoun, personal\n",
      "man ---> NOUN\n",
      "noun, singular or mass\n",
      "? ---> PUNCT\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# doc = sp(\"My name is Gayatri123s\")\n",
    "doc = sp('How are you man?')\n",
    "\n",
    "# print(doc[2].text,\"--->\",doc[2].pos_)\n",
    "# print(spacy.explain(doc[2].tag_))\n",
    "for token in doc:\n",
    "    print(token.text,\"--->\",token.pos_)\n",
    "    print(spacy.explain(token.tag_))\n",
    "#     print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos --> Tit VERB verb\n",
      "tag--> Tit VB verb, base form\n",
      "pos --> for ADP adposition\n",
      "tag--> for IN conjunction, subordinating or preposition\n",
      "pos --> tat NOUN noun\n",
      "tag--> tat NN noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# doc =sp(\"My name is Gayatri123s\")\n",
    "doc = sp('Tit for tat')\n",
    "\n",
    "for token in doc:\n",
    "    print('pos -->', token.text,token.pos_, spacy.explain(token.pos_))\n",
    "    print('tag-->', token.text,token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PRP$ pronoun, possessive\n",
      "name NN noun, singular or mass\n",
      "is VBZ verb, 3rd person singular present\n",
      "Gayatri123s NNP noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = sp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc = sp(\"My name is Gayatri123s\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My poss\n",
      "name nsubj\n",
      "is ROOT\n",
      "Gayatri123s attr\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc = sp(\"My name is Gayatri123s\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Xxxxx\n",
      "is xx\n",
      "looking xxxx\n",
      "at xx\n",
      "buying xxxx\n",
      "U.K. X.X.\n",
      "startup xxxx\n",
      "for xxx\n",
      "$ $\n",
      "1 d\n",
      "billion xxxx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "doc = sp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# doc = sp(\"My name is Gayatri\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_extension', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n",
      "My True\n",
      "My False\n",
      "My True\n",
      "name True\n",
      "name False\n",
      "name True\n",
      "is True\n",
      "is False\n",
      "is True\n",
      "demo True\n",
      "demo False\n",
      "demo True\n",
      "2534 False\n",
      "2534 True\n",
      "2534 True\n",
      "s89 False\n",
      "s89 False\n",
      "s89 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc = sp(\"My name is demo 2534 s89\")\n",
    "print(dir(doc[0]))\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    print(token.text,token.is_alpha)\n",
    "    print(token.text,token.is_digit)\n",
    "    print(token.text, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My True\n",
      "name True\n",
      "is True\n",
      "Gayatri123s False\n",
      "is True\n",
      "and True\n",
      "the True\n",
      "all True\n",
      "aditi False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc = nlp(\"My name is Gayatri123s is and the all aditi\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunking\n",
    "Now that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idea group nouns the words relation them\n",
      "The idea\n",
      "group nouns\n",
      "the words\n",
      "relation\n",
      "them\n",
      "inside if them === PRP\n"
     ]
    }
   ],
   "source": [
    "# text = \"Australian striker John hits century\"\n",
    "text = \"The idea is to group nouns with the words that are in relation to them\"\n",
    "# text = \"How are you, Snehal?\"\n",
    "doc = sp(text)\n",
    "print(*doc.noun_chunks)\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)  \n",
    "    for token in nc:\n",
    "#         print(token.tag_)\n",
    "#         if(token.tag_ == 'NN'):\n",
    "        if(token.tag_ == 'PRP'):\n",
    "            print('inside if', token.text,\"===\",token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Parts of Speech Tags\n",
    "Visualizing POS tags in a graphical way is extremely easy. The displacy module from the spacy library is used for this purpose. To visualize the POS tags inside the Jupyter notebook, you need to call the render method from the displacy module and pass it the spacy document, the style of the visualization, and set the jupyter attribute to True as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a7d3a7b8d20e49dd89f16fa0a2d9871d-0\" class=\"displacy\" width=\"530\" height=\"197.0\" direction=\"ltr\" style=\"max-width: none; height: 197.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"107.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"107.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"107.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">raining</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"107.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">today</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-0\" stroke-width=\"2px\" d=\"M70,62.0 C70,2.0 170.0,2.0 170.0,62.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,64.0 L62,52.0 78,52.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-1\" stroke-width=\"2px\" d=\"M190,62.0 C190,2.0 290.0,2.0 290.0,62.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M290.0,64.0 L298.0,52.0 282.0,52.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-2\" stroke-width=\"2px\" d=\"M310,62.0 C310,2.0 410.0,2.0 410.0,62.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a7d3a7b8d20e49dd89f16fa0a2d9871d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M410.0,64.0 L418.0,52.0 402.0,52.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# sen = sp(\"I like to play football. I hated it in my childhood though\")\n",
    "# sen = sp(\"It's raining today\")\n",
    "\n",
    "displacy.render(sen, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">On \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thursday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vijayawada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " police had house-arrested \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TDP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " MP \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kesineni Srinivas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MLA Buddha Venkanna\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".Ahead of the meeting, unprecedented security arrangements have been made in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amaravati\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " region, particularly in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mandadam village\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and the roads leading to the Secretariat at Velagapudi in.I just bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9 a.m.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " because the stock went up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    30%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3/4/2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " have $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " just \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    WSJ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "'</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "# doc = sp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "# doc = sp(\"I like to play football. I hated it in my childhood though\")\n",
    "# doc = sp(\"It was raining on Wednesday at 10 p.m. \")\n",
    "# doc = sp(\"Startups raised $320 million\")\n",
    "# doc = sp(\"AWAKE! FEAR! FIRE! FOES! AWAKE! FEAR! FIRE! FOES!AWAKE! AWAKE! -- J. R. R. Tolkien\")\n",
    "doc = sp(\"On Thursday, Vijayawada police had house-arrested TDP MP Kesineni Srinivas, MLA Buddha Venkanna.Ahead of the meeting, unprecedented security arrangements have been made in Amaravati region, particularly in Mandadam village and the roads leading to the Secretariat at Velagapudi in.I just bought 2 shares at 9 a.m. because the stock went up 30% in 3/4/2019 have $2 just 2 days according to the WSJ'\")\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we Need to Remove Stopwords?\n",
    "Quite an important question and one you must have in mind.\n",
    "\n",
    "Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['through', 'six', 'next', 'my', 'its', 'yourself', '‘re', 'otherwise', 'own', 'are']\n"
     ]
    }
   ],
   "source": [
    "# Stop words collection\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Lemmatization is the process of converting a word to its base form.\n",
      "\n",
      "['Lemmatization', 'process', 'converting', 'word', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# stop words removing\n",
    "# doc = sp(\"I like to play football. I hated it in my childhood\")\n",
    "doc = sp(\"Lemmatization is the process of converting a word to its base form.\")\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "print('Original Article: %s' % (doc))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming\n",
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She --> she\n",
      "was --> was\n",
      "hastily --> hastili\n",
      "running --> run\n",
      "away --> away\n",
      "from --> from\n",
      "the --> the\n",
      "scary --> scari\n",
      "bear --> bear\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "tokens = ['running', 'run', 'ran']\n",
    "sentence = sp('She was hastily running away from the scary bear')\n",
    "for token in sentence:\n",
    "#     print(token.text)\n",
    "    print(token.text + ' --> ' + stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON-\n",
      "be\n",
      "hastily\n",
      "run\n",
      "away\n",
      "from\n",
      "the\n",
      "scary\n",
      "bear\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# sentence = \"I like to play football. I hated it in my childhood\"\n",
    "# sentence = \"'compute', 'computer', 'computed', 'computing'\"\n",
    "# sentence = \"The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\"\n",
    "# sentence = \"They’re versioned and can be defined as a dependency in your requirements.txt\"\n",
    "sentence = \"She was hastily running away from the scary bear\"\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = sp(sentence)\n",
    "for token in doc:\n",
    "    print(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Entities\n",
    "In addition to tokenizing the documents to words, you can also find if the word is an entity such as a company, place, building, currency, institution, etc.\n",
    "\n",
    "Let's see a simple example of named entity recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5 = sp('Manchester United is looking to sign Harry Kane for $90 million') \n",
    "# sentence5 = sp('She was hastily running away from the scary bear')\n",
    "sentence5 = sp('When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in sentence5:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where named entity recognition comes to play. To get the named entities from a document, you have to use the ents attribute. Let's retrieve the named entities from the above sentence. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian\n",
      "Sebastian - NORP - Nationalities or religious or political groups\n",
      "Google\n",
      "Google - ORG - Companies, agencies, institutions, etc.\n",
      "2007\n",
      "2007 - DATE - Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence5.ents:\n",
    "#     print(dir(entity))\n",
    "    print(entity)\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
