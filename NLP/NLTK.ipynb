{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practioners have benefited from machine learning techniques to unlock meaning from large corpora, and in this class we’ll explore how to do that particularly with Python and with the Natural Language Toolkit (NLTK). \n",
    "\n",
    "NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications. \n",
    "\n",
    "## Quick Overview of NLTK\n",
    "NLTK stands for the Natural Language Toolkit and is written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). NTLK provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very pythonic \"batteries included\" view of Natural Language Processing.   \n",
    "\n",
    "As such, NLTK is perfect for researh driven (hypothesis driven) workflows for agile data science. Its suite of libraries includes:\n",
    "\n",
    "- tokenization, stemming, and tagging\n",
    "- chunking and parsing\n",
    "- language modeling\n",
    "- classification and clustering\n",
    "- logical semantics\n",
    "\n",
    "NLTK is a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. It is also important to understand what NLTK is _not_:\n",
    "\n",
    "- Production ready out of the box\n",
    "- Lightweight\n",
    "- Generally applicable\n",
    "- Magic\n",
    "\n",
    "NLTK provides a variety of tools that can be used to explore the linguistic domain but is not a lightweight dependency that can be easily included in other workflows, especially those that require unit and integration testing or other build processes. This stems from the fact that NLTK includes a lot of added code but also a rich and complete library of corpora that power the built-in algorithms. \n",
    "\n",
    "### The Good parts of NLTK\n",
    "\n",
    "- Preprocessing\n",
    "    - segmentation\n",
    "    - tokenization\n",
    "    - PoS tagging\n",
    "- Word level processing\n",
    "    - WordNet\n",
    "    - Lemmatization\n",
    "    - Stemming\n",
    "    - NGrams\n",
    "- Utilities\n",
    "    - Tree\n",
    "    - FreqDist\n",
    "    - ConditionalFreqDist\n",
    "    - Streaming CorpusReaders\n",
    "- Classification\n",
    "    - Maximum Entropy\n",
    "    - Naive Bayes\n",
    "    - Decision Tree\n",
    "- Chunking\n",
    "- Named Entity Recognition\n",
    "- Parsers Galore!\n",
    "\n",
    "### The Bad parts of NLTK\n",
    "\n",
    "- Syntactic Parsing\n",
    "\n",
    "    - No included grammar (not a black box)\n",
    "    - No Feature/Dependency Parsing\n",
    "    - No included feature grammar\n",
    "\n",
    "- The sem package\n",
    "    \n",
    "    - Toy only (lambda-calculus & first order logic)\n",
    "\n",
    "- Lots of extra stuff (heavyweight dependency)\n",
    "\n",
    "    - papers, chat programs, alignments, etc.\n",
    "\n",
    "Knowing the good and the bad parts will help you explore NLTK further - looking into the source code to extract the material you need, then moving that code to production. We will explore NLTK in more detail in the rest of this notebook. \n",
    "\n",
    "## Installing NLTK\n",
    "\n",
    "This notebook has a few dependencies, most of which can be installed via the python package manger - `pip`. \n",
    "\n",
    "1. Python 2.7 or later (anaconda is ok)\n",
    "2. NLTK\n",
    "3. The NLTK corpora \n",
    "4. The BeautifulSoup library\n",
    "5. The gensim libary\n",
    "\n",
    "Once you have Python and pip installed you can install NLTK as follows:\n",
    "\n",
    "    ~$ pip install nltk\n",
    "    \n",
    "    ~$ pip install matplotlib\n",
    "    \n",
    "    ~$ pip install beautifulsoup4\n",
    "    \n",
    "    ~$ pip install gensim\n",
    "\n",
    "Note that these will also install Numpy and Scipy if they aren't already installed. \n",
    "\n",
    "To download the corpora, open a python interperter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/synerzip/snap/jupyter/common/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /snap/jupyter/6/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "Error connecting to server: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip3: not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/synerzip/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open up a window with which you can download the various corpora and models to a specified location. For now, go ahead and download it all as we will be exploring as much of NLTK as we can. Also take note of the `download_directory` - you're going to want to know where that is so you can get a detailed look at the corpora that's included. I usually export an enviornment variable to track this:\n",
    "\n",
    "    ~$ export NLTK_DATA=/path/to/nltk_data\n",
    "    \n",
    "_Take a moment to explore what is in this directory_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Example Corpora\n",
    "\n",
    "NLTK ships with a variety of corpora, let's use a few of them to do some work. Get access to the text from _Moby Dick_ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.text.Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.text.Text` class is a wrapper around a sequence of simple (string) tokens - intended only for _the initial exploration of text_ usually via the Python REPL. It has the following methods:\n",
    "\n",
    "- common_contexts\n",
    "- concordance\n",
    "- collocations\n",
    "- count\n",
    "- plot\n",
    "- findall\n",
    "- index\n",
    "\n",
    "You shouldn't use this class in production level systems, but it is useful to explore (small) snippets of text in a meaningful fashion.\n",
    "\n",
    "The corcordance function performs a search for the given token and then also provides the surrounding context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 11 matches:\n",
      "er , one was of a most monstrous size . ... This came t\n",
      "ALMS . \" Touching that monstrous bulk of the whale or o\n",
      " a heathenish array of monstrous clubs and spears . Som\n",
      "ed , and wondered what monstrous cannibal and savage co\n",
      "vived the flood ; most monstrous and most mountainous !\n",
      "cout at Moby Dick as a monstrous fable , or still worse\n",
      " .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I\n",
      " In connexion with the monstrous pictures of whales , I\n",
      " upon those still more monstrous stories of them which \n",
      "n rummaged out of this monstrous cabinet there is no te\n"
     ]
    }
   ],
   "source": [
    "moby.concordance(\"monstrous\", 55, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some context surrounding a word, we can discover similar words, e.g. words that that occur frequently in the same context and with a similar distribution: Distributional similarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it he that queequeg stubb him there starbuck i what man and all then\n",
      "this me peleg which here you\n",
      "None\n",
      "very so exceedingly heartily a as good great extremely remarkably\n",
      "sweet vast amazingly\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (moby.similar(\"ahab\"))\n",
    "austen = nltk.text.Text(nltk.corpus.gutenberg.words('/home/synerzip/nltk_data/corpora/gutenberg/austen-sense.txt'))\n",
    "\n",
    "print (austen.similar(\"monstrous\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this takes a bit of time to build the index in memory, one of the reasons it's not suggested to use this class in production code. Now that we can do searching and similarity, find the common contexts of a set of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of_s to_the cried_give with_the murmured_gazing at_s to_s and_s\n",
      "cried_let cried_look by_s of_and in_the but_s cried_to\n"
     ]
    }
   ],
   "source": [
    "moby.common_contexts([\"ahab\", \"starbuck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your turn, go ahead and explore similar words and contexts - what does the common context mean?_\n",
    "\n",
    "NLTK also uses matplotlib and pylab to display graphs and charts that can show dispersions and frequency. This is especially interesting for the corpus of innagural addresses given by U.S. presidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/synerzip/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31b077ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.download('inaugural')\n",
    "inaugural = nltk.text.Text(nltk.corpus.inaugural.words())\n",
    "inaugural.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore much of the built in corpus, use the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists the various corpora and CorpusReader classes in the nltk.corpus module\n",
    "for name in dir(nltk.corpus):\n",
    "    if name.islower() and not name.startswith('_'): print name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a specific corpus, list the fileids that are available:\n",
    "print nltk.corpus.shakespeare.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print nltk.corpus.stopwords.fileids()\n",
    "nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "print string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These corpora export several vital methods:\n",
    "\n",
    "- paras (iterate through each paragraph)\n",
    "- sents (iterate through each sentence)\n",
    "- words (iterate through each word)\n",
    "- raw   (get access to the raw text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "print corpus.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print corpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print corpus.raw()[:200] # Be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn! Explore some of the text in the available corpora_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analyses\n",
    "\n",
    "In statistical machine learning approaches to NLP, the very first thing we need to do is count things - especially the unigrams that appear in the text and their relationships to each other. NLTK provides two very excellent classes to enable these frequency analyses:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n",
    "\n",
    "And these two classes serve as the foundation for most of the probability and statistical analyses that we will conduct.\n",
    "\n",
    "First we will compute the following:\n",
    "\n",
    "- The count of words\n",
    "- The vocabulary (unique words)\n",
    "- The lexical diversity (the ratio of word count to vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('reuters')\n",
    "reuters = nltk.corpus.reuters # Corpus of news articles\n",
    "counts  = nltk.FreqDist(reuters.words())\n",
    "vocab   = len(counts.keys())\n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print \"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print counts.most_common(40)  # The n most common tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print counts.max() # The most frequent token in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print counts.hapaxes()[0:10]  # A list of all hapax legomena "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.freq('stipulate') * 100 # percentage of the corpus for this token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.plot(200, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "categories = brown.categories()\n",
    "\n",
    "counts = nltk.ConditionalFreqDist(chain(*[[(cat, word) for word in brown.words(categories=cat)] for cat in categories]))\n",
    "\n",
    "for category, dist in counts.items():\n",
    "    vocab  = len(dist.keys())\n",
    "    tokens = sum(dist.values())\n",
    "    lexdiv = float(tokens) / float(vocab)\n",
    "    print \"%s: %i types with %i tokens and lexical diveristy of %0.3f\" % (category, vocab, tokens, lexdiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn: compute the conditional frequency distribution of bigrams in a corpus_\n",
    "\n",
    "Hint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in nltk.ngrams([\"The\", \"bear\", \"walked\", \"in\", \"the\", \"woods\", \"at\", \"midnight\"], 5):\n",
    "    print ngram\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "NLTK is great at the preprocessing of Raw text - it provides the following tools for dividing text into it's constituent parts:\n",
    "\n",
    "- `sent_tokenize`: a Punkt sentence tokenizer:\n",
    "\n",
    "    This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.  It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "    \n",
    "    However, Punkt is designed to learn parameters (a list of abbreviations, etc.) unsupervised from a corpus similar to the target domain. The pre-packaged models may therefore be unsuitable: use PunktSentenceTokenizer(text) to learn parameters from the given text.\n",
    "    \n",
    "    \n",
    "- `word_tokenize`: a Treebank tokenizer \n",
    "\n",
    "    The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This is the method that is invoked by ``word_tokenize()``.  It assumes that the text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "    \n",
    "\n",
    "- `pos_tag`: a maximum entropy tagger trained on the Penn Treebank\n",
    "\n",
    "    There are several other taggers including (notably) the BrillTagger as well as the BrillTrainer to train your own tagger or tagset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"Medical personnel returning to New York and New Jersey from the Ebola-riddled countries in West Africa will be automatically quarantined if they had direct contact with an infected person, officials announced Friday. New York Gov. Andrew Cuomo (D) and New Jersey Gov. Chris Christie (R) announced the decision at a joint news conference Friday at 7 World Trade Center. “We have to do more,” Cuomo said. “It’s too serious of a situation to leave it to the honor system of compliance.” They said that public-health officials at John F. Kennedy and Newark Liberty international airports, where enhanced screening for Ebola is taking place, would make the determination on who would be quarantined. Anyone who had direct contact with an Ebola patient in Liberia, Sierra Leone or Guinea will be quarantined. In addition, anyone who traveled there but had no such contact would be actively monitored and possibly quarantined, authorities said. This news came a day after a doctor who had treated Ebola patients in Guinea was diagnosed in Manhattan, becoming the fourth person diagnosed with the virus in the United States and the first outside of Dallas. And the decision came not long after a health-care worker who had treated Ebola patients arrived at Newark, one of five airports where people traveling from West Africa to the United States are encountering the stricter screening rules.\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text): \n",
    "    print sent\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print list(nltk.wordpunct_tokenize(sent))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print list(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these taggers work _pretty_ well - but you can (and should train them on your own corpora). \n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "We have an immense number of word forms as you can see from our various counts in the `FreqDist` above - it is helpful for many applications to normalize these word forms (especially applications like search) into some canonical word for further exploration. In English (and many other languages) - mophological context indicate gender, tense, quantity, etc. but these sublties might not be necessary:\n",
    "\n",
    "Stemming = chop off affixes to get the root stem of the word:\n",
    "\n",
    "    running --> run\n",
    "    flowers --> flower\n",
    "    geese   --> geese \n",
    "    \n",
    "Lemmatization = look up word form in a lexicon to get canonical lemma\n",
    "\n",
    "    women   --> woman\n",
    "    foxes   --> fox\n",
    "    sheep   --> sheep\n",
    "    \n",
    "There are several stemmers available:\n",
    "\n",
    "    - Lancaster (English, newer and aggressive)\n",
    "    - Porter (English, original stemmer)\n",
    "    - Snowball (Many langauges, newest)\n",
    "    \n",
    "The Lemmatizer uses the WordNet lexicon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print \" \".join(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the lemmatizer has to load the WordNet corpus which takes a bit.\n",
    "\n",
    "Typical normalization of text for use as features in machine learning models looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token\n",
    "\n",
    "print list(normalize(\"The eagle flies at midnight.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "print nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"John Smith is from the United States of America and works at Microsoft Research Labs\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap the Stanford NER system, which many of you are also probably used to using."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
